use anyhow::{anyhow, Result};
use candle_core::{quantized::gguf_file, Device, Tensor};
use candle_transformers::generation::LogitsProcessor;
use candle_transformers::models::quantized_qwen2 as model;
use hf_hub::{api::tokio::Api, Repo, RepoType};
use std::io::Write;
use tokenizers::Tokenizer; // print!ã§é€æ¬¡å‡ºåŠ›ã™ã‚‹ãŸã‚

pub struct LlmEngine {
    model: model::ModelWeights,
    tokenizer: Tokenizer,
    device: Device,
}

impl LlmEngine {
    pub async fn new() -> Result<Self> {
        println!("â³ Initializing Candle Engine...");

        // 1. CUDAãƒã‚§ãƒƒã‚¯
        let device = Device::new_cuda(0).unwrap_or_else(|e| {
            println!("âš ï¸ CUDA Error: {}. Falling back to CPU.", e);
            Device::Cpu
        });
        println!("ğŸš€ Device: {:?}", device);

        // 2. ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        let api = Api::new()?;

        // tokenizer ã¯é€šå¸¸ç‰ˆãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰å–å¾—ï¼ˆGGUFã«ã¯å«ã¾ã‚Œãªã„ï¼‰
        let tokenizer_repo = api.repo(Repo::new(
            "Qwen/Qwen2.5-3B-Instruct".to_string(),
            RepoType::Model,
        ));
        println!("ğŸ“¥ Downloading/Loading tokenizer...");
        let tokenizer_path = tokenizer_repo.get("tokenizer.json").await?;
        let tokenizer = Tokenizer::from_file(tokenizer_path).map_err(|e| anyhow!(e))?;

        // GGUF weights ã¯ GGUF ãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰å–å¾—
        let gguf_repo = api.repo(Repo::new(
            "Qwen/Qwen2.5-3B-Instruct-GGUF".to_string(),
            RepoType::Model,
        ));
        println!("ğŸ“¥ Downloading/Loading model weights (q8_0)...");
        let model_path = gguf_repo.get("qwen2.5-3b-instruct-q8_0.gguf").await?;

        let mut file = std::fs::File::open(&model_path)?;
        let gguf_content = gguf_file::Content::read(&mut file)?;

        // ãƒ¡ãƒ¢ãƒªã¸ã®ãƒ­ãƒ¼ãƒ‰
        let model = model::ModelWeights::from_gguf(gguf_content, &mut file, &device)?;

        println!("âœ… Engine Ready.");
        Ok(Self {
            model,
            tokenizer,
            device,
        })
    }

    pub fn generate(&mut self, system: &str, user: &str) -> Result<String> {
        // Qwenç‰¹æœ‰ã®ChatMLãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        let prompt_str = format!(
            "<|im_start|>system\n{}<|im_end|>\n<|im_start|>user\n{}<|im_end|>\n<|im_start|>assistant\n",
            system, user
        );

        // ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
        let tokens = self
            .tokenizer
            .encode(prompt_str, true)
            .map_err(|e| anyhow!(e))?;
        let prompt_tokens = tokens.get_ids().to_vec();

        let mut generated_tokens = Vec::new();

        // ç”Ÿæˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        let mut logits_processor = LogitsProcessor::new(1337, Some(0.7), Some(0.95));
        let eos_token = *self
            .tokenizer
            .get_vocab(true)
            .get("<|im_end|>")
            .unwrap_or(&151645);

        // 1. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå…¨ä½“ã‚’ forward ã—ã¦ KV cache ã‚’æ§‹ç¯‰
        let input = Tensor::new(prompt_tokens.as_slice(), &self.device)?.unsqueeze(0)?;
        let logits = self.model.forward(&input, 0)?;
        let logits = logits.squeeze(0)?; // [vocab_size]

        // æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        let mut next_token = logits_processor.sample(&logits)?;
        let mut pos = prompt_tokens.len();

        // 2. ç”Ÿæˆãƒ«ãƒ¼ãƒ—
        for _ in 0..1000 {
            generated_tokens.push(next_token);

            // ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤ºï¼ˆ100ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ï¼‰
            if generated_tokens.len() % 100 == 0 {
                print!(".");
                std::io::stdout().flush()?;
            }

            if next_token == eos_token {
                break;
            }

            // Forward Pass (1ãƒˆãƒ¼ã‚¯ãƒ³ãšã¤)
            let input = Tensor::new(&[next_token], &self.device)?.unsqueeze(0)?;
            let logits = self.model.forward(&input, pos)?;
            let logits = logits.squeeze(0)?;

            // Sampling
            next_token = logits_processor.sample(&logits)?;
            pos += 1;
        }

        // æœ€çµ‚çš„ãªæ–‡å­—åˆ—ã‚’è¿”ã™
        let result = self
            .tokenizer
            .decode(&generated_tokens, true)
            .map_err(|e| anyhow!(e))?;
        Ok(result)
    }
}
